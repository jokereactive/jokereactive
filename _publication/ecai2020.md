---
layout: page_publication
name: Learning Physics Intuition Models for Non-Disruptive Object Extraction from Clutter
type: paper
authors: Sarthak Ahuja, Henny Admoni, Aaron Steinfeld
track: Research Track
submission: Full Research Paper
acceptance: Submitted
_url: http://ecai2020.eu/
project_url: 
thumbnail_url: thumbnail_ecai2019.png
type: conference
conference_url: http://ecai2020.eu/
venue: ECAI
projectimage: sample.png
weight: 0
reference: 
code:
slides: 
poster: 
database: 
demo: 
special: 
year: 2020
excerpt: Additional content
---
Robots operating in human environments must be careful when executing their manipulation skills. This requires robots to reason about the repercussions of their actions on other objects in the environment. Humans can visually inspect their surroundings and gain a physical intuition about how likely it is that a particular object can be safely manipulated (i.e., cause no disruption in the rest of the scene). Existing work has shown the ability of deep convolutional neural networks to learn intuitive physics over images generated in simulation and determine the stability of the scene. In this paper, we extend these physics intuition models to the task of assessing safe object extraction by conditioning the visual images on specific objects in the scene during training. We further explore methods for aggregating multiple views of a scene to increase the model's accuracy for scenes that contain either a large number of objects or unstructured object arrangements. Our results in a simulated object extraction task show that with our proposed method, physics intuition models can be used to accurately inform a robot of which objects can be safely extracted and from which direction to extract them.