---
layout: page_project
name: Multi-Sensor Data Fusion for Human Activity Recognization
intro: 
teamsize: 2
teammembers: Anchita Goel
guide: Dr. Richa Singh
course: Machine Learning
projectimage: msdfhar.png
weight: 4
reference: http://sarthakahuja.org/public/docs/report_multisensor.pdf
category: Research
code: https://github.com/jokereactive/MDF-Human-Activity-Recognition
database: https://drive.google.com/a/iiitd.ac.in/folderview?id=0B6sTEEC-di3sQjYwZFJkRi1HOGM&usp=sharing
demo:
slides: 
poster: 
special: First Prize, Technical Paper Presentation, Cogenesis 2016, Delhi Technological University
technology: Matlab, OpenCV, Optical Flow, Signal Processing, Data Fusion
period: Aug'15-Dec'15
excerpt: 
---
Human activity recognition is a well known area of
research in pervasive computing, which involves detecting activity
of an individual by using various types of sensors. This finds great
utility in the context of human-centric problems in the real world
not only for purposes of tracking ones daily activities but also in
monitoring activities of others - like the elderly, patrol officers,
etc for purposes of health-care and security. With the growth of
interest in AI, such a system can provide useful information to
make the agent much more intelligent and aware about the user,
thus giving a more personalized experience. Several technologies
have been used to get estimates of a personâ€™s activity like sensors
found in smartphones(accelerometer, gyroscope, magnetometer
etc.), egocentric cameras, other wearable sensors to measure vital
signs like heart rate, respiration rate and skin temperature (apart
from the same data provided by smartphones), worn on different
parts of the body like chest, wrist, ankles, environment sensors to
measure humidity, audio level, temperature etc. The activities that
can be recognised are daily activities like walking, lying down,
sitting, standing, running, travelling. However, to the best of our
knowledge we have come across no work where a fusion of these
sensors and egocentric cameras has been put to use. In this paper
we explore the suggested fusion of sensors and share the results
obtained. Our fusion approach shows significant improvement
over using both the chosen sensors independently.