---
layout: post
title:  "Learning Vision-Based Physics Intuition Models for Non-Disruptive Object Extraction"
date:   2020-02-01 00:00:00 +00:00
image: /images/thumbnail_iros2020.png
categories: research
author: "Sarthak Ahuja"
subtitle: "Nominated for Best Student Paper"
authors: "<strong>Sarthak Ahuja</strong>, Henny Admoni, Aaron Steinfeld"
venue: "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
poster: 
paper: http://harp.ri.cmu.edu/assets/pubs/sarthak_iros_2020.pdf
course: 
---
Robots operating in human environments must be careful, when executing their manipulation skills, not to disturb nearby objects. This requires robots to reason about the effect of their manipulation choices by accounting for the support relationships among objects in the scene. Humans do this in part by visually assessing their surroundings and using physics intuition for how likely it is that a particular object can be safely manipulated (i.e., cause no disruption in the rest of the scene). Existing work has shown that deep convolutional neural networks can learn intuitive physics over images generated in simulation and determine the stability of a scene in the real world. In this paper, we extend these physics intuition models to the task of assessing safe object extraction by conditioning the visual images on specific objects in the scene. Our results, in both simulation and real-world settings, show that with our proposed method, physics intuition models can be used to inform a robot of which objects can be safely extracted and from which direction to extract them.
